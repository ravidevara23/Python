{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"19TL9k_vV49xvyabYwtodLnJUEntEHNJD","authorship_tag":"ABX9TyMYE8PTHntIuoNjNj4lYl0t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\n","\n","https://www.datacamp.com/tutorial/adaboost-classifier-python\n","\n","https://www.datacamp.com/tutorial/guide-to-the-gradient-boosting-algorithm\n","\n","https://www.geeksforgeeks.org/regularization-in-machine-learning/\n","\n","https://www.analyticsvidhya.com/blog/2021/08/ensemble-stacking-for-machine-learning-and-deep-learning/"],"metadata":{"id":"31j3CGRPh3FI"}},{"cell_type":"markdown","source":["## Steps of Gradient Boosting with Learning Rate\n","\n","**Initial Prediction:** The model starts with an initial prediction, often the mean of the target values.\n","\n","**First Iteration:** The model fits a weak learner to the residuals of the initial prediction. The predictions of this weak learner are multiplied by the learning rate and added to the initial prediction.\n","\n","**Subsequent Iterations:** Each new weak learner is fitted to the residuals of the current model. The predictions of these weak learners are multiplied by the learning rate and added to the current model's predictions to improve accuracy.\n","\n","**Final Prediction:** After a specified number of iterations, the model combines all the weak learners' predictions (each adjusted by the learning rate) to make the final prediction."],"metadata":{"id":"EhZ8qsJrrfsY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KcnKKOxJh2eI"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","education_level = [1, 3, 4, 3]\n","Age = [20, 64, 55, 35]\n","Salary = [45, 60, 70, 90]\n","\n","df = pd.DataFrame({\n","    'Education Level': education_level,\n","    'Age': Age,\n","    'Salary': Salary\n","})\n","\n","X = df[['Education Level', 'Age']]\n","y = df['Salary']\n","\n","\n","\n"]},{"cell_type":"code","source":["# Initialize Gradient Boosting Regressor\n","gb = GradientBoostingRegressor(n_estimators=7, learning_rate=1, max_depth=1, random_state=42)\n","gb.fit(X, y)\n","\n","# Get predictions and residuals for each stage\n","predictions = np.zeros((4, 7))\n","residuals = np.zeros((4, 7))\n","\n","for i, y_pred in enumerate(gb.staged_predict(X)):\n","    predictions[:, i] = y_pred\n","    residuals[:, i] = y - y_pred\n","\n","\n","# Create a DataFrame to store the data\n","data = pd.DataFrame({\n","    'Education Level': education_level,\n","    'Age': Age,\n","    'Salary': Salary,\n","    'Prediction Stage 1': predictions[:, 0],\n","    'Residual Stage 1': residuals[:, 0],\n","    'Prediction Stage 2': predictions[:, 1],\n","    'Residual Stage 2': residuals[:, 1],\n","    'Prediction Stage 3': predictions[:, 2],\n","    'Residual Stage 3': residuals[:, 2],\n","    'Prediction Stage 4': predictions[:, 3],\n","    'Residual Stage 4': residuals[:, 3],\n","    'Prediction Stage 5': predictions[:, 4],\n","    'Residual Stage 5': residuals[:, 4],\n","    'Prediction Stage 6': predictions[:, 5],\n","    'Residual Stage 6': residuals[:, 5],\n","    'Prediction Stage 7': predictions[:, 6],\n","    'Residual Stage 7': residuals[:, 6],\n","    'Final Prediction': gb.predict(X)\n","})\n","\n","data\n","\n","# print(data)\n","# # Save the DataFrame to an Excel file\n","# file_path = '/content/drive/MyDrive/Colab Notebooks/Gradient_Boosting_Example.xlsx'\n","# data.to_excel(file_path, index=False)\n","\n","# print(f\"Excel file generated and saved to {file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"id":"zXxW6j1OQW-A","executionInfo":{"status":"ok","timestamp":1737487539909,"user_tz":240,"elapsed":270,"user":{"displayName":"Fouad Kayali","userId":"06378865004183699418"}},"outputId":"bbe8fc9f-b2f3-4d33-f4c6-9c0d57f647bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Education Level  Age  Salary  Prediction Stage 1  Residual Stage 1  \\\n","0                1   20      45           45.000000          0.000000   \n","1                3   64      60           73.333333        -13.333333   \n","2                4   55      70           73.333333         -3.333333   \n","3                3   35      90           73.333333         16.666667   \n","\n","   Prediction Stage 2  Residual Stage 2  Prediction Stage 3  Residual Stage 3  \\\n","0           53.333333         -8.333333           45.000000          0.000000   \n","1           65.000000         -5.000000           67.777778         -7.777778   \n","2           65.000000          5.000000           67.777778          2.222222   \n","3           81.666667          8.333333           84.444444          5.555556   \n","\n","   Prediction Stage 4  Residual Stage 4  Prediction Stage 5  Residual Stage 5  \\\n","0           47.592593         -2.592593           45.000000          0.000000   \n","1           60.000000          0.000000           60.864198         -0.864198   \n","2           70.370370         -0.370370           71.234568         -1.234568   \n","3           87.037037          2.962963           87.901235          2.098765   \n","\n","   Prediction Stage 6  Residual Stage 6  Prediction Stage 7  Residual Stage 7  \\\n","0           46.049383         -1.049383           45.000000          0.000000   \n","1           59.814815          0.185185           60.164609         -0.164609   \n","2           70.185185         -0.185185           70.534979         -0.534979   \n","3           88.950617          1.049383           89.300412          0.699588   \n","\n","   Final Prediction  \n","0         45.000000  \n","1         60.164609  \n","2         70.534979  \n","3         89.300412  "],"text/html":["\n","  <div id=\"df-f4494742-e1f8-48d5-803a-6c1fb99a15be\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Education Level</th>\n","      <th>Age</th>\n","      <th>Salary</th>\n","      <th>Prediction Stage 1</th>\n","      <th>Residual Stage 1</th>\n","      <th>Prediction Stage 2</th>\n","      <th>Residual Stage 2</th>\n","      <th>Prediction Stage 3</th>\n","      <th>Residual Stage 3</th>\n","      <th>Prediction Stage 4</th>\n","      <th>Residual Stage 4</th>\n","      <th>Prediction Stage 5</th>\n","      <th>Residual Stage 5</th>\n","      <th>Prediction Stage 6</th>\n","      <th>Residual Stage 6</th>\n","      <th>Prediction Stage 7</th>\n","      <th>Residual Stage 7</th>\n","      <th>Final Prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>45</td>\n","      <td>45.000000</td>\n","      <td>0.000000</td>\n","      <td>53.333333</td>\n","      <td>-8.333333</td>\n","      <td>45.000000</td>\n","      <td>0.000000</td>\n","      <td>47.592593</td>\n","      <td>-2.592593</td>\n","      <td>45.000000</td>\n","      <td>0.000000</td>\n","      <td>46.049383</td>\n","      <td>-1.049383</td>\n","      <td>45.000000</td>\n","      <td>0.000000</td>\n","      <td>45.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>64</td>\n","      <td>60</td>\n","      <td>73.333333</td>\n","      <td>-13.333333</td>\n","      <td>65.000000</td>\n","      <td>-5.000000</td>\n","      <td>67.777778</td>\n","      <td>-7.777778</td>\n","      <td>60.000000</td>\n","      <td>0.000000</td>\n","      <td>60.864198</td>\n","      <td>-0.864198</td>\n","      <td>59.814815</td>\n","      <td>0.185185</td>\n","      <td>60.164609</td>\n","      <td>-0.164609</td>\n","      <td>60.164609</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4</td>\n","      <td>55</td>\n","      <td>70</td>\n","      <td>73.333333</td>\n","      <td>-3.333333</td>\n","      <td>65.000000</td>\n","      <td>5.000000</td>\n","      <td>67.777778</td>\n","      <td>2.222222</td>\n","      <td>70.370370</td>\n","      <td>-0.370370</td>\n","      <td>71.234568</td>\n","      <td>-1.234568</td>\n","      <td>70.185185</td>\n","      <td>-0.185185</td>\n","      <td>70.534979</td>\n","      <td>-0.534979</td>\n","      <td>70.534979</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>35</td>\n","      <td>90</td>\n","      <td>73.333333</td>\n","      <td>16.666667</td>\n","      <td>81.666667</td>\n","      <td>8.333333</td>\n","      <td>84.444444</td>\n","      <td>5.555556</td>\n","      <td>87.037037</td>\n","      <td>2.962963</td>\n","      <td>87.901235</td>\n","      <td>2.098765</td>\n","      <td>88.950617</td>\n","      <td>1.049383</td>\n","      <td>89.300412</td>\n","      <td>0.699588</td>\n","      <td>89.300412</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4494742-e1f8-48d5-803a-6c1fb99a15be')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f4494742-e1f8-48d5-803a-6c1fb99a15be button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f4494742-e1f8-48d5-803a-6c1fb99a15be');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-fb09a3ed-6726-4eaf-a333-b17e6017531e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fb09a3ed-6726-4eaf-a333-b17e6017531e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-fb09a3ed-6726-4eaf-a333-b17e6017531e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_fc88c7c9-9a62-4fc5-ad06-8153c6555bcd\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_fc88c7c9-9a62-4fc5-ad06-8153c6555bcd button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('data');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data","summary":"{\n  \"name\": \"data\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Education Level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 20,\n        \"max\": 64,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          64,\n          35,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Salary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 45,\n        \"max\": 90,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          60,\n          90,\n          45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction Stage 1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.166666666666664,\n        \"min\": 45.0,\n        \"max\": 73.33333333333333,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          73.33333333333333,\n          45.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Residual Stage 1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.472191289246473,\n        \"min\": -13.333333333333329,\n        \"max\": 16.66666666666667,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -13.333333333333329,\n          16.66666666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction Stage 2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.656741810198488,\n        \"min\": 53.333333333333336,\n        \"max\": 81.66666666666666,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          53.333333333333336,\n          65.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Residual Stage 2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.934920476158726,\n        \"min\": -8.333333333333336,\n        \"max\": 8.333333333333343,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -5.0,\n          8.333333333333343\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction Stage 3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.19947034647037,\n        \"min\": 45.0,\n        \"max\": 84.44444444444444,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          45.0,\n          67.77777777777779\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Residual Stage 3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.665577237325319,\n        \"min\": -7.777777777777786,\n        \"max\": 5.555555555555557,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -7.777777777777786,\n          5.555555555555557\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction Stage 4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.69570515565961,\n        \"min\": 47.59259259259259,\n        \"max\": 87.03703703703704,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          60.0,\n          87.03703703703704\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Residual Stage 4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.2831162973959156,\n        \"min\": -2.592592592592588,\n        \"max\": 2.962962962962962,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          2.962962962962962\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction Stage 5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.020277871635315,\n        \"min\": 45.0,\n        \"max\": 87.90123456790123,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          60.864197530864196,\n          87.90123456790123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Residual Stage 5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.4917340708141484,\n        \"min\": -1.2345679012345698,\n        \"max\": 2.098765432098773,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.864197530864196,\n          2.098765432098773\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction Stage 6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.076506433622338,\n        \"min\": 46.04938271604939,\n        \"max\": 88.9506172839506,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          59.81481481481481,\n          88.9506172839506\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Residual Stage 6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8700565642743266,\n        \"min\": -1.0493827160493865,\n        \"max\": 1.0493827160493936,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.18518518518519045,\n          1.0493827160493936\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prediction Stage 7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.603557034408862,\n        \"min\": 45.0,\n        \"max\": 89.30041152263374,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          60.16460905349794,\n          89.30041152263374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Residual Stage 7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5172759296286701,\n        \"min\": -0.5349794238683216,\n        \"max\": 0.6995884773662624,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.16460905349794075,\n          0.6995884773662624\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Final Prediction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.603557034408862,\n        \"min\": 45.0,\n        \"max\": 89.30041152263374,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          60.16460905349794,\n          89.30041152263374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["\n","In Gradient Boosting, the learning_rate is a hyperparameter that you set before training the model, rather than being calculated during the training process. It is a scaling factor applied to each step of the boosting process to control the contribution of each tree to the final model. The learning_rate essentially controls the weight of each weak learner in the ensemble.\n","\n","**Key Points About learning_rate**\n","\n","  * **Fixed Value:** Typically, the learning_rate is a fixed value chosen by the user before training the model. It does not change during training.\n","\n","  * **Impact on Model:** A smaller learning_rate makes the model training slower but can lead to better generalization because the model takes smaller steps towards the final solution, reducing the risk of overfitting.\n","\n","  * **Trade-Off with n_estimators:** There is usually a trade-off between the learning_rate and the number of boosting stages (n_estimators). A lower learning_rate often requires a larger number of boosting stages to maintain model performance."],"metadata":{"id":"SwvO5-zrr8YO"}},{"cell_type":"markdown","source":["## Explain the parameter learning_rate"],"metadata":{"id":"xSiE0qtqEP47"}},{"cell_type":"markdown","source":["**Explanation of the Example below**\n","\n","**Dataset:** We generate a simple dataset with a linear relationship and some noise.\n","\n","**Gradient Boosting Regressor:** Two models are created with different learning_rate values (0.01 and 0.1).\n","\n","**Fitting Models:** Both models are trained on the training data.\n","Predictions and Evaluation: Predictions are made on the test set, and the Mean\n","\n","**Squared Error (MSE)** is calculated to compare the performance.\n","Trade-Off and Choosing learning_rate\n","\n","**Lower learning_rate:** Typically requires more trees (n_estimators) to achieve the same performance, leading to longer training times but potentially better generalization.\n","\n","**Higher learning_rate:** Each tree has a larger impact, potentially leading to faster convergence but higher risk of overfitting.\n","\n","**Choosing the learning_rate:** Choosing the right learning_rate often involves experimenting with different values and performing cross-validation to find the best balance between training time and model performance. Common values to try range from 0.01 to 0.3.\n","\n","By understanding and properly tuning the learning_rate, you can significantly improve the performance and robustness of a Gradient Boosting model."],"metadata":{"id":"Rjx9bp0Xsacs"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.model_selection import train_test_split\n","\n","# Example dataset\n","np.random.seed(42)\n","X = np.random.rand(100, 1) * 10\n","y = 2 * X.squeeze() + 1 + np.random.randn(100) * 2  # Linear relation with noise\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize Gradient Boosting Regressor with different learning rates\n","gb_low_lr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.01, max_depth=1, random_state=42)\n","gb_high_lr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42)\n","\n","# Fit the models\n","gb_low_lr.fit(X_train, y_train)\n","gb_high_lr.fit(X_train, y_train)\n","\n","# Predict\n","y_pred_low_lr = gb_low_lr.predict(X_test)\n","y_pred_high_lr = gb_high_lr.predict(X_test)\n","\n","# Compare the performance\n","from sklearn.metrics import mean_squared_error\n","print(f\"Low learning rate MSE: {mean_squared_error(y_test, y_pred_low_lr)}\")\n","print(f\"High learning rate MSE: {mean_squared_error(y_test, y_pred_high_lr)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6VH6C_T5sSPd","executionInfo":{"status":"ok","timestamp":1737487633686,"user_tz":240,"elapsed":271,"user":{"displayName":"Fouad Kayali","userId":"06378865004183699418"}},"outputId":"c12441e1-cff5-4cfb-d1c7-7e1af6f2ac42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Low learning rate MSE: 10.745529378608582\n","High learning rate MSE: 2.707721931894042\n"]}]},{"cell_type":"markdown","source":["## Comparison Between Gradient Boosting and XGBoost"],"metadata":{"id":"IPtw37WWDG3J"}},{"cell_type":"markdown","source":["\n","### Gradient Boosting (GBM):\n","\n","**Implementation:** Traditional Gradient Boosting methods are implemented in libraries like scikit-learn.\n","\n","**Speed:** Generally slower due to less optimization and lack of advanced features.\n","\n","**Regularization:** Basic regularization techniques are available.\n","\n","**Handling Missing Values:** Generally, the data needs to be preprocessed to handle missing values.\n","\n","**Flexibility:** Supports basic hyperparameters tuning.\n","\n","### XGBoost (Extreme Gradient Boosting):\n","\n","**Implementation:** An optimized implementation of gradient boosting designed to be highly efficient, flexible, and portable.\n","\n","**Speed:** Faster due to optimizations like parallel processing, tree pruning, and efficient memory usage.\n","\n","**Regularization:** More advanced regularization (L1 & L2) to prevent overfitting.\n","\n","**Handling Missing Values:** Automatically handles missing values in the dataset.\n","\n","**Flexibility:** Offers more parameters and better control over the model training process.\n","\n","**Scalability:** Scales well to large datasets."],"metadata":{"id":"-pPpDKZpCnr5"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.ensemble import GradientBoostingRegressor\n","import xgboost as xgb\n","\n","# Load dataset\n","california_housing = fetch_california_housing()\n","X, y = california_housing.data, california_housing.target"],"metadata":{"id":"-1bu8dTuV6cw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize models\n","gbm = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n","xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n","\n","# Fit models\n","gbm.fit(X_train, y_train)\n","xgb_model.fit(X_train, y_train)\n","\n","# Predict\n","y_pred_gbm = gbm.predict(X_test)\n","y_pred_xgb = xgb_model.predict(X_test)\n","\n","# Evaluate performance\n","mse_gbm = mean_squared_error(y_test, y_pred_gbm)\n","mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n","\n","print(f\"Gradient Boosting MSE: {mse_gbm}\")\n","print(f\"XGBoost MSE: {mse_xgb}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_R-PU-juDcLZ","executionInfo":{"status":"ok","timestamp":1737487748741,"user_tz":240,"elapsed":13676,"user":{"displayName":"Fouad Kayali","userId":"06378865004183699418"}},"outputId":"4a8523c2-f094-4af0-d78f-cc0acf2b80b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient Boosting MSE: 0.2939973248643864\n","XGBoost MSE: 0.29522676196268116\n"]}]},{"cell_type":"markdown","source":["### Pros and Cons\n","\n","### Gradient Boosting (GBM):\n","\n","**Pros:**\n","* Simple to use with basic regularization.\n","* Integrated into many libraries (e.g., scikit-learn).\n","\n","**Cons:**\n","* Slower and less efficient for large datasets.\n","* Requires more manual handling of missing values.\n","\n","### XGBoost:\n","\n","**Pros:**\n","* Faster and more efficient due to advanced optimizations.\n","* Automatically handles missing values.\n","* Better regularization techniques.\n","* Scales well with large datasets.\n","\n","**Cons:**\n","* More complex and requires understanding of additional parameters.\n","* Can be overkill for small datasets or simple tasks."],"metadata":{"id":"0vLbFWgFDzt9"}},{"cell_type":"markdown","source":["## Key Parameters in Gradient Boosting (GBM):"],"metadata":{"id":"eTKyFcCrHQb4"}},{"cell_type":"markdown","source":["**n_estimators:**\n","\n","* Description: The number of boosting stages to be run.\n","* Effect: Increasing the number of estimators typically improves performance, but also increases training time and can lead to overfitting.\n","\n","**learning_rate:**\n","\n","* Description: The contribution of each tree to the final model.\n","* Effect: A lower learning rate requires more trees to reach the same level of performance. It controls the step size at each iteration while moving toward a minimum of the loss function.\n","\n","**max_depth:**\n","\n","* Description: The maximum depth of each individual tree.\n","* Effect: Limits the number of nodes in the tree, which helps control overfitting. Higher values lead to more complex models.\n","\n","**min_samples_split:**\n","\n","* Description: The minimum number of samples required to split an internal node.\n","* Effect: Helps control overfitting. Lower values allow the model to learn more, but can lead to overfitting.\n","\n","**min_samples_leaf:**\n","\n","* Description: The minimum number of samples required to be at a leaf node.\n","* Effect: Controls overfitting by ensuring a minimum number of observations at each leaf.\n","\n","**subsample:**\n","\n","* Description: The fraction of samples to be used for fitting each tree.\n","* Effect: Helps control overfitting by introducing randomness. Values less than 1.0 lead to Stochastic Gradient Boosting.\n","\n","**max_features:**\n","\n","* Description: The number of features to consider when looking for the best split.\n","* Effect: Reducing the number of features can help prevent overfitting and improve the speed of the algorithm.\n","\n","**loss:**\n","\n","* Description: The loss function to be optimized. Common choices are 'deviance' for classification (log-loss) and 'ls' for regression (least squares).\n","* Effect: Defines how the error is calculated and therefore how the model is optimized.\n","\n","**criterion:**\n","\n","* Description: The function to measure the quality of a split. Common choices are 'friedman_mse', 'mse', and 'mae'.\n","* Effect: Influences how the quality of a split is measured."],"metadata":{"id":"xy4Y2Q6AHWmX"}},{"cell_type":"markdown","source":["## Key Parameters of XGBoost"],"metadata":{"id":"UMAi_dbvIFKL"}},{"cell_type":"markdown","source":["\n","**General Parameters**\n","\n","***booster:***\n","\n","* Description: Specifies which booster to use: gbtree, gblinear, or dart.\n","* Default: gbtree\n","* Effect: Determines the type of model to be used. 'gbtree' and 'dart' use tree-based models, while 'gblinear' uses linear models.\n","\n","***nthread:***\n","\n","* Description: Number of parallel threads used to run XGBoost.\n","* Default: Maximum number of threads available if not set.\n","* Effect: Controls the parallelism of the algorithm, potentially speeding up training.\n","\n","***random_state:***\n","\n","* Description: Random number seed.\n","* Effect: Ensures reproducibility of the results.\n","Booster Parameters\n","\n","**Parameters for Tree Booster (gbtree and dart)**\n","***eta (learning_rate):***\n","\n","* Description: Step size shrinkage used to prevent overfitting.\n","* Default: 0.3\n","* Range: [0, 1]\n","* Effect: Smaller values make the model more robust by shrinking the weights of new trees.\n","\n","***max_depth:***\n","\n","* Description: Maximum depth of a tree.\n","* Default: 6\n","* Range: [0, ∞]\n","* Effect: Controls overfitting; deeper trees capture more patterns but can lead to overfitting.\n","\n","***min_child_weight:***\n","\n","* Description: Minimum sum of instance weight (hessian) needed in a child.\n","* Default: 1\n","* Range: [0, ∞]\n","* Effect: Higher values prevent overfitting by requiring a minimum weight for leaf nodes.\n","\n","***gamma:***\n","\n","* Description: Minimum loss reduction required to make a further partition on a leaf node.\n","* Default: 0\n","* Range: [0, ∞]\n","* Effect: Larger values make the algorithm more conservative.\n","\n","***subsample:***\n","\n","* Description: Subsample ratio of the training instances.\n","* Default: 1\n","* Range: (0, 1]\n","* Effect: Reduces overfitting by sampling a fraction of the training data.\n","\n","***colsample_bytree:***\n","\n","* Description: Subsample ratio of columns when constructing each tree.\n","* Default: 1\n","* Range: (0, 1]\n","* Effect: Reduces overfitting by sampling a fraction of the features.\n","\n","***colsample_bylevel:***\n","\n","* Description: Subsample ratio of columns for each level.\n","* Default: 1\n","* Range: (0, 1]\n","* Effect: Reduces overfitting by sampling a fraction of features at each level.\n","\n","***lambda (reg_lambda):***\n","\n","* Description: L2 regularization term on weights.\n","* Default: 1\n","* Range: [0, ∞]\n","* Effect: Prevents overfitting by penalizing large weights.\n","\n","***alpha (reg_alpha):***\n","\n","* Description: L1 regularization term on weights.\n","* Default: 0\n","* Range: [0, ∞]\n","* Effect: Prevents overfitting by inducing sparsity in the model.\n","\n","**Parameters for Linear Booster (gblinear)**\n","***lambda (reg_lambda):***\n","\n","* Description: L2 regularization term on weights.\n","* Default: 0\n","* Range: [0, ∞]\n","* Effect: Similar to ridge regression.\n","\n","***alpha (reg_alpha):***\n","\n","* Description: L1 regularization term on weights.\n","* Default: 0\n","* Range: [0, ∞]\n","* Effect: Similar to Lasso regression.\n","\n","***updater:***\n","\n","* Description: Choice of algorithm to fit linear model.\n","* Options: ‘shotgun’, ‘coord_descent’\n","* Effect: Controls the optimization algorithm.\n","\n","**Learning Task Parameters**\n","\n","***objective:***\n","\n","* Description: Specifies the learning task and the corresponding objective function.\n","* Default: 'reg' for regression.\n","\n","* Options: ‘reg\n","’, ‘binary\n","’, ‘multi\n","’, etc.\n","\n","* Effect: Defines the type of problem (regression, binary classification, multi-class classification, etc.)\n","\n","***eval_metric:***\n","\n","* Description: Evaluation metric(s) for validation data.\n","* Default: Determined by objective.\n","* Options: 'rmse', 'mae', 'logloss', 'error', 'merror', etc.\n","* Effect: Measures the performance of the model during training and evaluation.\n","\n","***base_score:***\n","\n","* Description: Initial prediction score of all instances, global bias.\n","* Default: 0.5\n","* Effect: The starting value for the model's predictions."],"metadata":{"id":"wp0VKSzhJYZa"}}]}